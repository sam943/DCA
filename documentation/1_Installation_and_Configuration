Installation and Configuration - 15%

  Complete Docker Installation on Multiple Platforms (CentOS/Red Hat)
  -------------------------------------------------------------------
	- yum install -y yum-utils device-mapper-persistent-data lvm2
	- yum config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo
	- yum update
	-  yum install -y docker-ce
 	- systemctl enable docker && systemctl start docker && systemctl status docker
cd /var/run/
   	- ls -l docker.sock
   	- useradd user1
	- usermod -aG docker user1 

  Complete Docker Installation on Multiple Platforms (Debian/Ubuntu)
  ------------------------------------------------------------------	
	- apt-get install apt-transport-https ca-certificates curl software-properties-common
	- curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
	- sudo apt-get repository "deb [arch-amd64] https://downlaod.docker.com/linux/ubuntu $(lsb_release -cs) stable"
	- apt-get update
	- apt-get install docker-ce
	- systemctl status docker
	- usermod -aG docker user1
	- /var/run/docker.sock 

  Selecting a Storage Driver
  --------------------------
	- Docker supports several different storage drivers, using a pluggable architecture. The storage driver controls how images and containers are stored and managed on your Docker host.
	- try to use the storage driver with the best overall performance and stability in the most usual scenarios.
		- overlay2 is preferred, followed by overlay. Neither of these requires extra configuration. overlay2 is the default choice for Docker CE.
		- devicemapper is next, but requires direct-lvm for production environments, because loopback-lvm, while zero-configuration, has very poor performance
	- devicemapper is officially supported on centos/fedora
	Linux distribution	Recommended storage drivers
	Docker CE on Ubuntu	aufs, devicemapper, overlay2 (Ubuntu 14.04.4 or later, 16.04 or later), overlay, zfs, vfs
	Docker CE on Debian	aufs, devicemapper, overlay2 (Debian Stretch), overlay, vfs
	Docker CE on CentOS	devicemapper, vfs
	Docker CE on Fedora	devicemapper, overlay2 (Fedora 26 or later, experimental), overlay (experimental), vf	

	- If you need to change the storage driver, then you can create daemon.json under /etc/docker and add the storage driver
	- The default storage driver that comes along with docker-ce installation is overlay
	- By the adding below json object into daemon.json and restarting the docker service will change the storage driver
	- JSON object 
		cat /etc/docker/daemon.json
			{
				"storage-driver" : "devicemapper"
			}	
	- restart docker service
		systemctl restart docker.service
	- check the storage driver
		[root@localhost documentation]# docker info | grep Storage
		WARNING: devicemapper: usage of loopback devices is strongly discouraged for production use.
		         Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.
		Storage Driver: devicemapper
	- devicemapper is offically supported,recommended storage driver
	- If you are ever changing the storage driver, it is best recommeneded to take the backup/export of the existing images and import after changing the storage drivers
	- Once you change the storage driver to devicemapper, you will notice devicemapper directory under /var/lib/docker/.
	- Any images downloaded will also increase size of this devicemapper

   Configuring Logging Drivers (Syslog, JSON-File, etc.)
   -----------------------------------------------------
	- configuring the logging drivers as similar to as storage drivers
	- Docker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers.	
	- To configure the Docker daemon to default to a specific logging driver, set the value of log-driver to the name of the logging driver in the daemon.json

 	- docker info | grep "Logging" will provide the logging driver	
	- To change the default log driver from json-format to syslog, update below JSONdoc into the /etc/docker/daemon.json
	- JSON object
                cat /etc/docker/daemon.json
                        {
                                "log-driver" : "syslog"
			        "log-opts" : {
                        			"syslog-address" :  "udp://<docker-host-ip>:514"
			                     }	
                        } 
	- And also uncomment module(load="imudp") & input(type="imudp" port="514") in /etc/rsyslog.conf
	- Restart the rsyslog and docker services
	- docker info | grep "Logging" will provide the logging driver
	- After the above configuration if you start the docker container, docker logs will be logged into /var/log/messages
	-  docker run -it --log-driver <driver_name> <image_name:tag> # you can also specify or overwrite the default log-driver per container also
	- docker inspect -f '{{.HostConfig.LogConfig.Type}}' <CONTAINER> # check log-driver on container

	- Docker provides two modes for delivering messages from the container to the log driver:
		- (default) direct, blocking delivery from container to drive
		- non-blocking delivery that stores log messages in an intermediate per-container ring buffer for consumption by driver	
 	- Below is an exampe for non-blocking mode
		docker run -it --log-opt mode=non-blocking --log-opt max-buffer-size=4m <image_id:tag>	
	- The max-buffer-size log option controls the size of the ring buffer used for intermediate message storage when mode is set to non-blocking. max-buffer-size defaults to 1 megabyte.	
	

  Setting Up Swarm
  -----------------
	- Setting up docker swarm mode cluster.
	- (manager)
	- docker swarm is going to contain one or more nodes or one or more managers
	- nodes does the work and managers manage the workload 
	- Setup manager to deploy containers and services
		- docker swarn init --advertise-addr <SERVER_IP>
	  	- Once init is executed, then below worker token is created
		- docker swarm join --token SWMTKN-1-<TOKEN> <SERVER_IP>:<PORT> # Note this token for adding nodes to the manager
		- docker swarm join-token worker # you can use this command on the manager to generate docker join-token used for adding the nodes
	- If you need to add another node as a additional manager , you will need to generate manager token
		- docker swarm join-token manager # token to join another manager"

  Setting Up Swarm (Add Nodes)
  ----------------------------
	
	- Once the manager is setup if you run the docker node ls command, you should see host that manager/Leader an
	- docker system info # Would provide the more information about how many worker and manager are added
	- run "docker swarm join-token worker" on worker nodes 1 and nodes 2 to join the nodes to this manager
	- If you get the below error when you try to add the swarm worker nodes to manager
		Error response from daemon: rpc error: code = 14 desc = grpc: the connection is unavailable
	  then
		disable firewall on manager (systemctl stop firewalld) or allow the port in firewall
	- After added node1 and node2 to the manager and run docker node ls, You should see below information

	ID       HOSTNAME                 		STATUS     AVAILABILITY     MANAGER STATUS
	<ID> *   docker-host-1.localdomain              Ready      Active    	    Leader
	<ID>     docker-worker-node-1.localdomain       Ready      Active
	<ID>     docker-worker-node-2.localdomain       Ready      Active

  Setting Up Swarm (Backup and Restore)
  -------------------------------------

	- docker service create --name <service_name> -p 80:80 --replicas 2 <imageid:tag> # Create a service and define the number of repicas
  	- docker service ls # will list the services running
	- docker service ps backupweb  # will show the no.of replicas running for the service
	- Backup Swarm
		- Stop the docker server one of the  manager (sudo systemctl docker stop)	
		- Take the backup for /var/lib/docker/swarm directory as swarm-bkup.tar
		- Copy over the backup swarm directory to new server
		- Untar this swarm-bkup.tar into /var/lib/docker/swarm
		- Then run, docker swarm init --force-new-cluster

  Outline the Sizing Requirements Prior to Installation
  -------------------------------------
     - Disk space,Memory and CPU will need to be considered when planning
     - Concurreny
     - 
     - UCP(Universal Control Panel) is a UI for managing docker-managers or DTRs(Docker trusted Registery)
     	- Requirement for UCP
		- 8GB memory for manager or DTR, 4GB memory for workers and 3GB diskspace
		- 4vCPUs
		
v	- Performance Considerations(Timings)
		-  Raft Consensus between manager nodes - 3000 - Not Configurable
		-  Gossip Protocal for overlay networking - 5000 - Not Configurable
		-  etcd - 500 - yes
		-  RethinkDB - 10000 - Not Configurable
		-  Standalone-swarm - 10000 - Not Configurable
		
	-  Docker EE Includes
		- Docker Engine with Support
		- Docker Trusted Registry
		- Universal Control Plane
	- Compatibility
		- Docker Engine 17.06+
		- DTR 2.3+
		- UCP 2.2+

  Setup and Configure UCP and DTR for Secure Cluster Management
  ------------------------------------------------------------
	-- Install UCP (Universal Control Plane)

		 docker container run --rm -it --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp:2.2.4 install --host-address <HOST_IP> --interactive
	 	 - The above command will install the UCP server. But, before you setup the UCP you will some basic swarm environment ( like manager and worker ), so you see and manager nodes on  the UCP UI and manage the same way you do on the command line
		 - Once you setup UCP, you get the https:<HOST_NAME/IP>:443 URL that you can access the UCP UI
		- Once you login you  see the dashboard for swarm environment and also manage the opertion through
		- If you want to interate DTR with UCP, will need to get the comamnd details from "Advanced Settings" -> DTR -> Insecure-TLS(if you want to ignore tls
		Once you select the above the below command similar command is generated.	
	 -- Install DTR (Docker Trusted Registery)
	 	docker run -it --rm docker/dtr install  --ucp-node docker-swarm-manager-2.localdomain  --ucp-username admin  --ucp-url https://192.168.99.103:443 --ucp-insecure-tls --replica-http-port 8080 --replica-https-port 8443 
		- The above command will install the DTR and intgreate with UCP.
		- You can change the default port (80, 443) with --replica-http-port  and --replica-https-port
		- Once the above the command is executed use teh https:<HOST_IP>:8443 to access the DTR
		- DTR password will be same as UCP 		

  Complete Backup for UCP and DTR
  --------------------------------
	-- Backup UCP
	docker container run --log-driver none --rm -i --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp backup > backup.tar
	docker container run --log-driver none --rm -i --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp backup --id <ID_RETURNED FROM ABOVE> > backup.tar # without interactive mode

	-- Restore UCP
	docker container run --log-driver none --rm -i --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp restore  < backup.tar

	-- Backup DTR
	docker run -i --rm docker/dtr backup --ucp-insecure-tls --ucp-url https://192.168.99.103:443 --ucp-username admin  > dtr-backup.tar

	-- Restore DTR
	docker run -i --rm docker/dtr backup --ucp-insecure-tls --ucp-url https://192.168.99.103:443 --ucp-username admin  < dtr-backup.tar

	Note : When you run backup on DTR, DTR is offline until the backup is completed. This means that all docker push, pull, run that uses the DTR will fail during this time. So It is always advised to backup during midnights
